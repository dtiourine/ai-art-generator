{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-31T13:31:45.165955Z",
     "start_time": "2024-08-31T13:31:45.158323Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from art_generator.config import PROCESSED_DATA_DIR\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "LOAD_MODEL_PATH = '20500_epoch'\n",
    "SAVE_MODEL_PATH = '25000_epoch'"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T13:31:45.651681Z",
     "start_time": "2024-08-31T13:31:45.631555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_size = 256\n",
    "\n",
    "dataset = ImageFolder(root=PROCESSED_DATA_DIR, \n",
    "                      transform=transforms.Compose([transforms.Resize(image_size),\n",
    "                                                    transforms.CenterCrop(image_size),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "                                                    ]))"
   ],
   "id": "de0e2b04d82ee16b",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T13:31:46.164346Z",
     "start_time": "2024-08-31T13:31:46.160333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "96e7bea34bb5cbe0",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T13:31:47.346540Z",
     "start_time": "2024-08-31T13:31:46.557782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from architectures import Generator, Critic\n",
    "\n",
    "# Loading the generator\n",
    "def load_generator(path, GeneratorClass):\n",
    "    checkpoint = torch.load(path)\n",
    "    generator = GeneratorClass()  # You need to define the generator architecture\n",
    "    generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return generator\n",
    "\n",
    "# Loading the generator\n",
    "def load_critic(path, CriticClass):\n",
    "    checkpoint = torch.load(path)\n",
    "    critic = CriticClass()  # You need to define the generator architecture\n",
    "    critic.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return critic\n",
    "\n",
    "netG = load_generator( LOAD_MODEL_PATH + \"/generator.pth\", Generator)\n",
    "netC = load_critic(LOAD_MODEL_PATH + \"/critic.pth\", Critic)\n",
    "netG.to(device)\n",
    "netC.to(device)"
   ],
   "id": "99b8a04a673b05cb",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T13:31:48.325435Z",
     "start_time": "2024-08-31T13:31:48.314665Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Architecture Params (also in architecture.py)\n",
    "\n",
    "nz = 100\n",
    "ngf = 64\n",
    "nc = 3\n",
    "ndf = 64\n",
    "\n",
    "# Gradient penalty function\n",
    "def compute_gradient_penalty(critic, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN-GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    critic_interpolates = critic(interpolates)\n",
    "    fake = torch.ones(real_samples.size(0), 1, device=device, requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=critic_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# Wasserstein loss\n",
    "def wasserstein_loss(y_pred, y_true):\n",
    "    return torch.mean(y_true * y_pred)\n",
    "\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 5e-5\n",
    "beta1 = 0.5\n",
    "n_critic = 5  # Number of critic iterations per generator iteration\n",
    "lambda_gp = 10  # Gradient penalty lambda hyperparameter\n",
    "\n",
    "# Setup RMSProp optimizers for both G and C\n",
    "optimizerD = optim.RMSprop(netC.parameters(), lr=lr)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr=lr)"
   ],
   "id": "2e713a6d668029a4",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:18:15.171308Z",
     "start_time": "2024-08-31T13:32:06.002787Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training Loop\n",
    "num_epochs = 4500\n",
    "n_critic = 5  # number of critic iterations per generator iteration\n",
    "lambda_gp = 10  # Gradient penalty lambda hyperparameter\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "# Gradient penalty function\n",
    "def compute_gradient_penalty(critic, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN-GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand(real_samples.size(0), 1, 1, 1, device=device)\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    critic_interpolates = critic(interpolates)\n",
    "    # Create a tensor of ones with the same shape as critic_interpolates\n",
    "    fake = torch.ones(critic_interpolates.size(), device=device, requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=critic_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network\n",
    "        ###########################\n",
    "        for _ in range(n_critic):\n",
    "            netC.zero_grad()\n",
    "            \n",
    "            # Train with real\n",
    "            real_images = data[0].to(device)\n",
    "            b_size = real_images.size(0)\n",
    "            \n",
    "            # Train with fake\n",
    "            noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "            fake = netG(noise)\n",
    "            \n",
    "            # Critic loss\n",
    "            real_validity = netC(real_images).view(-1)\n",
    "            fake_validity = netC(fake.detach()).view(-1)\n",
    "            \n",
    "            # Gradient penalty\n",
    "            gradient_penalty = compute_gradient_penalty(netC, real_images.data, fake.data)\n",
    "            \n",
    "            # Wasserstein loss\n",
    "            d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + lambda_gp * gradient_penalty\n",
    "            \n",
    "            d_loss.backward()\n",
    "            optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # Generate fake images\n",
    "        fake = netG(noise)\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        fake_validity = netC(fake).view(-1)\n",
    "        g_loss = -torch.mean(fake_validity)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     d_loss.item(), g_loss.item()))\n",
    "\n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(g_loss.item())\n",
    "        D_losses.append(d_loss.item())\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "        iters += 1"
   ],
   "id": "bfe70f57e45295fe",
   "execution_count": 49,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:26:24.656136Z",
     "start_time": "2024-08-31T20:26:24.564455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "f550956e74490480",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:26:30.495900Z",
     "start_time": "2024-08-31T20:26:29.753835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(), (1, 2, 0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1], (1, 2, 0)))\n",
    "plt.show()"
   ],
   "id": "602070fe2c10315a",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:26:49.662326Z",
     "start_time": "2024-08-31T20:26:49.658300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Saving the generator\n",
    "def save_model(model, path):\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_architecture': str(model),  # Save architecture as string\n",
    "        # Add any other information you want to save\n",
    "    }, path)"
   ],
   "id": "32ce0d58ed8906f8",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T20:27:29.157951Z",
     "start_time": "2024-08-31T20:27:28.639968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_model(netG, SAVE_MODEL_PATH + '/generator.pth')\n",
    "save_model(netC, SAVE_MODEL_PATH + '/critic.pth')"
   ],
   "id": "272c97c0f8856b92",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "88fb78035ceb1527",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
